name: Export incremental Snowflake changes
on:
 workflow_dispatch:
   inputs:
     schemas:
       description: "Space-separated schema list"
       required: true
       default: "COMETL_SFDC_CONTROL COMETL_SFDC_STAGING COMETL_SFDC_SYNC COMETL_SFDC_REPLICATION COMETL_SFDC_LANDING"
jobs:
 export:
   runs-on: ubuntu-latest
   permissions:
     contents: write
   steps:
     - name: Checkout
       uses: actions/checkout@v4
       with:
         fetch-depth: 0
     - name: Install Java (Liquibase runtime)
       uses: actions/setup-java@v4
       with:
         distribution: temurin
         java-version: "17"
     - name: Install Liquibase
       run: |
         set -euo pipefail
         LIQUIBASE_VERSION="4.29.2"
         curl -sL "https://github.com/liquibase/liquibase/releases/download/v${LIQUIBASE_VERSION}/liquibase-${LIQUIBASE_VERSION}.tar.gz" -o /tmp/liquibase.tgz
         sudo mkdir -p /opt/liquibase
         sudo tar -xzf /tmp/liquibase.tgz -C /opt/liquibase
         sudo ln -sf /opt/liquibase/liquibase /usr/local/bin/liquibase
         liquibase --version
     # âœ… FIX: Liquibase does NOT accept --privateKeyFile / --privateKeyFilePassword flags.
     # We instead convert .p8 -> unencrypted pkcs8 and pass it via JDBC connection property private_key_file.
     - name: Recreate and convert Snowflake private key (.p8 -> .pk8)
       env:
         KEY_B64: ${{ secrets.SNOWFLAKE_PRIVATE_KEY_B64 }}
         KEY_PASS: ${{ secrets.SNOWFLAKE_PRIVATE_KEY_PASSPHRASE }}
       run: |
         set -euo pipefail
         # 1) Recreate encrypted .p8 from GitHub Secret
         echo "$KEY_B64" | base64 --decode > snowflake_key.p8
         chmod 600 snowflake_key.p8
         # 2) Convert encrypted .p8 to unencrypted PKCS8 key (Snowflake JDBC-friendly)
         #    This uses the passphrase to decrypt the .p8, and writes an unencrypted pk8.
         openssl pkcs8 \
           -in snowflake_key.p8 \
           -out snowflake_key.pk8 \
           -passin pass:"$KEY_PASS" \
           -nocrypt
         chmod 600 snowflake_key.pk8
     - name: Export incremental changes -> new branch
       env:
         SF_USER: ${{ secrets.SNOWFLAKE_USER }}
         SF_JDBC_URL: ${{ secrets.SNOWFLAKE_JDBC_URL }}   # NOTE: should NOT include schema; we append it
         SCHEMAS: ${{ inputs.schemas }}
       run: |
         set -euo pipefail
         TIMESTAMP=$(date +"%Y%m%d_%H%M")
         BRANCH="db-export-${TIMESTAMP}"
         git config user.name "github-actions[bot]"
         git config user.email "github-actions[bot]@users.noreply.github.com"
         git checkout -b "$BRANCH"
         mkdir -p snowflake/snapshots
         mkdir -p snowflake/ddls snowflake/dmls snowflake/procs
         # Routes generated Liquibase formatted SQL into DDL/DML/PROCS buckets.
         # DROP is ALWAYS blocked here.
         route_changes () {
           RAW="$1"
           DDL="$2"
           DML="$3"
           PROCS="$4"
           awk '
             /^--liquibase formatted sql/ {print > ddl;print > dml;print > procs;next}
             /^--changeset/              {print > ddl;print > dml;print > procs;next}
             /^--/                       {print > ddl;print > dml;print > procs;next}
             {
               line=toupper($0)
               # BLOCK DROP always
               if (line ~ /^DROP[[:space:]]+/) next
               # PROCS / FUNCTIONS
               if (line ~ /(CREATE|ALTER).* (PROCEDURE|FUNCTION)/) { print > procs; next }
               # DML
               if (line ~ /^(INSERT|UPDATE|DELETE|MERGE|COPY)[[:space:]]+/) { print > dml; next }
               # DDL (and grants)
               if (line ~ /(CREATE|ALTER|GRANT|REVOKE)/) { print > ddl; next }
               # fallback
               print > ddl
             }
           ' ddl="$DDL" dml="$DML" procs="$PROCS" "$RAW"
           # Delete empty outputs
           for f in "$DDL" "$DML" "$PROCS"; do
             if [ ! -s "$f" ] || ! grep -Eqi "CREATE|ALTER|GRANT|REVOKE|INSERT|UPDATE|DELETE|MERGE|COPY|PROCEDURE|FUNCTION" "$f"; then
               rm -f "$f" || true
             fi
           done
         }
         CHANGES=false
         for SCHEMA in $SCHEMAS; do
           echo "---- Processing schema: $SCHEMA ----"
           SNAPSHOT="snowflake/snapshots/${SCHEMA}.json"
           RAW="/tmp/${TIMESTAMP}_${SCHEMA}_raw.sql"
           # âœ… FIX: pass keypair auth via JDBC param private_key_file (NOT liquibase CLI flags)
           # Make sure your SNOWFLAKE_JDBC_URL already contains warehouse/role/db as needed.
           LIVE_URL="${SF_JDBC_URL}&schema=${SCHEMA}&private_key_file=$(pwd)/snowflake_key.pk8"
           # First run: create baseline snapshot only
           if [ ! -f "$SNAPSHOT" ]; then
             echo "Creating baseline snapshot for $SCHEMA..."
             liquibase \
               --url="$LIVE_URL" \
               --username="$SF_USER" \
               snapshot --snapshotFormat=json --outputFile="$SNAPSHOT"
             git add "$SNAPSHOT"
             CHANGES=true
             continue
           fi
           # Reverse diff: offline snapshot (Git) vs live Snowflake DEV
           liquibase \
             --url="offline:snowflake?snapshot=$SNAPSHOT" \
             --referenceUrl="$LIVE_URL" \
             --referenceUsername="$SF_USER" \
             diffChangeLog --changeLogFile="$RAW"
           # Skip if no meaningful content
           if [ ! -s "$RAW" ] || ! grep -Eqi "CREATE|ALTER|INSERT|UPDATE|DELETE|MERGE|COPY|PROCEDURE|FUNCTION" "$RAW"; then
             echo "No changes for $SCHEMA"
             rm -f "$RAW"
             continue
           fi
           DDL_OUT="snowflake/ddls/${TIMESTAMP}_${SCHEMA}.sql"
           DML_OUT="snowflake/dmls/${TIMESTAMP}_${SCHEMA}.sql"
           PROC_OUT="snowflake/procs/${TIMESTAMP}_${SCHEMA}.sql"
           route_changes "$RAW" "$DDL_OUT" "$DML_OUT" "$PROC_OUT"
           # Update snapshot checkpoint so next run is incremental
           liquibase \
             --url="$LIVE_URL" \
             --username="$SF_USER" \
             snapshot --snapshotFormat=json --outputFile="$SNAPSHOT"
           git add "$SNAPSHOT"
           [ -f "$DDL_OUT" ] && git add "$DDL_OUT"
           [ -f "$DML_OUT" ] && git add "$DML_OUT"
           [ -f "$PROC_OUT" ] && git add "$PROC_OUT"
           CHANGES=true
           rm -f "$RAW"
         done
         if [ "$CHANGES" = false ]; then
           echo "No incremental changes detected."
           exit 0
         fi
         if git diff --cached --quiet; then
           echo "Nothing staged to commit."
           exit 0
         fi
         git commit -m "Incremental Snowflake export (DDL+DML+PROCS) ${TIMESTAMP}"
         git push --set-upstream origin "$BRANCH"
         echo "âœ… Branch pushed: $BRANCH"
         echo "ðŸ‘‰ Next: Create PR manually and merge to dev/qa/prod."